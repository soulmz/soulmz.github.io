---
layout:     post
title:      kubeadm1.14.x HA高可用部署
subtitle:   kubernetes(kubeadm安装方式) 1.14集群 三主一从
date:       2019-05-22
author:     soulmz
header-img: img/post-bg-2015.jpg
catalog: true 						
tags:								
    - kubernetes
---

# kubeadm 1.14.2 HA 高可用部署


## 环境准备

本次部署的网络信息：

> - Cluster IP CIDR: 10.244.0.0/16
> - Service Cluster IP CIDR: 10.96.0.0/12
> - Service DNS IP: 10.96.0.10
> - DNS DN: cluster.local
> - Kubernetes API VIP: 10.10.110.100
> - Kubernetes Ingress VIP: traefik 反向代理

## 节点配置

> 配置 2C 3G
>
> 系统信息: cenos 7.6 
>
> 内核版本: Linux 5.1.5-1.el7.elrepo.x86_64

| 节点名称 | IP            | 角色                 | 备注     |
| -------- | ------------- | -------------------- | -------- |
| VIP      | 10.10.110.100 | HA                   | 负载服务 |
| master1  | 10.10.110.101 | master keepalived ha |          |
| master2  | 10.10.110.102 | master keepalived ha |          |
| master3  | 10.10.110.103 | master keepalived ha |          |
| worker1  | 10.10.110.104 | worker               |          |

## 所有节点操作

### Hostname

按照上面节点名称定义，配置对应的主机名。(修改完成后，退出会话。重新连接就可以看到修改后的信息了)

`hostnamectl set-hostname master1`

### master1节点 ssh 互信

… 配置master1节点免密登录到 所有其他节点。

### 升级内核

> 因为目前市面上包管理下内核版本会很低,安装docker后无论centos还是ubuntu会有如下bug,4.15的内核依然存在

`kernel:unregister_netdevice: waiting for lo to become free. Usage count = 1`

所以建议先升级内核

perl是内核的依赖包,如果没有就安装下

`[ ! -f /usr/bin/perl ] && yum install perl -y`

- 升级内核需要使用 elrepo 的yum 源,首先我们导入 elrepo 的 key并安装 elrepo 源

```shell
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
```

- 查看可用的内核

`yum --disablerepo="*" --enablerepo="elrepo-kernel" list available  --showduplicates`

> 在yum的ELRepo源中,mainline 为最新版本的内核,安装kernel

ipvs依赖于nf_conntrack_ipv4内核模块,4.19包括之后内核的rpm里没有,所以这里我安装其他版本的内核
下面链接可以下载到其他归档版本的

- ubuntuhttp://kernel.ubuntu.com/~kernel-ppa/mainline/
- RHELhttp://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/

下面是ml的内核和上面归档内核版本任选其一的安装方法

1. 自选版本内核安装方法

```shell
export Kernel_Vsersion=4.18.16-1
 4.20.0-1
wget  http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/kernel-ml{,-devel}-${Kernel_Vsersion}.el7.elrepo.x86_64.rpm
yum localinstall -y kernel-ml*
```

2. 最新内核安装

```shell
yum --disablerepo="*" --enablerepo="elrepo-kernel" list available  --showduplicates | grep -Po '^kernel-ml.x86_64\s+\K\S+(?=.el7)'
# 安装最新内核
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm ;yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y

...下面是输出
4.18.16-1
4.19.0-1
...
```

修改内核启动顺序,默认启动的顺序应该为1,升级以后内核是往前面插入,为0（如果每次启动时需要手动选择哪个内核,该步骤可以省略）

`grub2-set-default  0 && grub2-mkconfig -o /etc/grub2.cfg`

使用下面命令看看确认下是否启动默认内核指向上面安装的内核

`grubby --default-kernel`

- (可选,目前不推荐开启。采坑了) docker官方的内核检查脚本建议`(RHEL7/CentOS7: User namespaces disabled; add 'user_namespace.enable=1' to boot command line)`,使用下面命令开启

```shell
grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"
```

- 重启加载新内核

`reboot`

### 安装常用工具或者开发包

```shell
yum -y install epel-release
yum -y install bridge-utils tcpdump screen lrzsz net-tools python-devel gcc wget curl zip unzip nc telnet vim bind-utils openssl-devel yum -y install yum-plugin-priorities tcpdump screen net-tools  gcc wget curl lrzsz zip unzip nc telnet bind-utils rsync vim traceroute sysstat perf iotop iftop strace dstat htop pciutils mtr tree git lsof nmap sudo ntpdate bzip2 gzip xz cmake autoconf automake pcre pcre-devel zlib zlib-devel libselinux-python python-simplejson nethogs nload iptraf multitail tmux atop saidar bmon libcurl-devel libattr-devel python-devel openssl-devel openldap-devel readline-devel gmp-devel libmcrypt-devel mhash-devel libxslt-devel libjpeg-devel freetype-devel libxml2-devel zlib-devel glib2-devel bzip2-devel ncurses-devel e2fsprogs-devel krb5-devel libidn-devel libffi-devel yum-utils socat keepalived ipvsadm bash-com*

modprobe br_netfilter
```

### IPVS

所有机器选择需要开机加载的内核模块,以下是 ipvs 模式需要加载的模块并设置开机自动加载 

> 注：4.19 开始模块 nf_conntrack需要加载

```shell
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
ipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack"
for kernel_module in \${ipvs_modules}; do
 /sbin/modinfo -F filename \${kernel_module} > /dev/null 2>&1
 if [ $? -eq 0 ]; then
 /sbin/modprobe \${kernel_module}
 fi
done
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep ip_vs
```

所有机器需要设定`/etc/sysctl.d/k8s.conf`的系统参数。

```shell
cat <<EOF > /etc/sysctl.d/k8s.conf
vm.swappiness = 0
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

cat <<EOF > /etc/sysctl.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system

sysctl -p /etc/sysctl.d/k8s.conf
```

### hosts配置

```shell
sudo cat > /etc/hosts <<EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
# master 3个
10.10.110.101 master1
10.10.110.102 master2
10.10.110.103 master3
# VIP
10.10.110.100 k8s-lb.soulmz.local
# 节点
10.10.110.104 worker1
EOF
```

### Docker安装

```shell
 
 # 查看docker版本 
 yum list docker-ce --showduplicates|sort -r
 
 # 删除旧docker版本
 sudo yum -y remove docker \
                   docker-client \
                   docker-client-latest \
                   docker-common \
                   docker-latest \
                   docker-latest-logrotate \
                   docker-logrotate \
                   docker-selinux \
                   docker-engine-selinux \
                   docker-engine \
                   docker-ce \
                   docker-ce-cli \
                   docker-ce-selinux
 
 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
 # 指定安装 18.06.1
 yum -y install docker-ce-18.06.3.ce
 # 安装最新
  yum -y install docker-ce
 # docker提示
 yum install -y epel-release bash-completion && cp /usr/share/bash-completion/completions/docker /etc/bash_completion.d/
 
 mkdir -p /etc/docker/
 cat > /etc/docker/daemon.json <<EOF
 {
   "registry-mirrors": ["https://hub-mirror.c.163.com", "https://docker.mirrors.ustc.edu.cn"],
   "exec-opts": ["native.cgroupdriver=systemd"],
   "storage-driver": "overlay2",
   "storage-opts": [
     "overlay2.override_kernel_check=true"
   ],
   "max-concurrent-downloads": 20,
   "log-driver": "json-file",
   "log-opts": {
     "max-size": "100m",
     "max-file": "3"
   }
 }
 EOF
 
 systemctl daemon-reload && systemctl enable docker
 # 编辑systemctl的Docker启动文件
 sed -i "13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT" /usr/lib/systemd/system/docker.service
 systemctl daemon-reload && systemctl start docker
```

### 服务器时间同步

```shell
yum install -y ntpdate
ntpdate -u cn.ntp.org.cn
#如果想定时执行ntpdate进行时间同步，可以通过crontab来进行：
crontab -e
#输入以下内容，每小时的第19分钟做一次时间同步：
05 * * * * /usr/sbin/ntpdate cn.ntp.org.cn >> /var/log/ntpdate.log
```

### 关闭防火墙

```shell
systemctl stop firewalld.service
systemctl disable firewalld.service
```

### 禁用Selinux

```shell
sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
setenforce 0
```

### 禁用Swap

```shell
swapoff -a
sed -i 's/.*swap.*/#&/' /etc/fstab

free 可以查看到swap等于0
```

### ulimit设置[可选]

```shell
cat <<EOF > /etc/security/limits.d/90-nproc.conf
*          soft    nproc     50000
*          hard    nproc     60000
*          soft    nofile    1024000
*          hard    nofile    1024000
root       soft    nproc     unlimited
EOF
```

### 安装kubeadm、kubectl、kubelet

> 这里使用 阿里云的源安装

```shell
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 查看可使用版本
yum list kubelet --showduplicates
# 如果有旧的 可以选择删除或更新
sudo yum remove -y kubelet kubeadm kubectl
# 安装最新的
sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
# 设置开机自启动并运行kubelet
systemctl enable kubelet && systemctl start kubelet

# 指定版本安装方式
yum install -y kubelet-1.13.5-0.x86_64 kubectl-1.13.5-0.x86_64 kubeadm-1.13.5-0.x86_64

```

### 补全提示 kubeadm 和 kubectl

```shell
当前shell生效：
source <(kubectl completion bash)
永久生效：
echo "source <(kubectl completion bash)" >> ~/.bashrc

当前shell生效：
source <(kubeadm completion bash)
永久生效：
echo "source <(kubeadm completion bash)" >> ~/.bashrc

source ~/.bashrc
```

## Master节点操作

### keepalived VIP 配置

```shell
yum install -y keepalived

# master1
[root@master1 ~]# cat /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
}
vrrp_script check_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -60
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state MASTER
    interface ens33
    mcast_src_ip 10.10.110.101
    virtual_router_id 50
    priority 150
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass a3ViZWFkbWNsdXN0ZXI=
    }
    virtual_ipaddress {
        10.10.110.100
    }
    track_script {
       check_apiserver
    }
}

# master2
[root@master2 ~]# cat /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
}
vrrp_script check_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -60
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    mcast_src_ip 10.10.110.102
    virtual_router_id 50
    priority 125
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass a3ViZWFkbWNsdXN0ZXI=
    }
    virtual_ipaddress {
        10.10.110.100
    }
    track_script {
       check_apiserver
    }
}

# master3
[root@master3 ~]# cat /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
}
vrrp_script check_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -60
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state BACKUP
    interface ens33
    mcast_src_ip 10.10.110.103
    virtual_router_id 50
    priority 100
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass a3ViZWFkbWNsdXN0ZXI=
    }
    virtual_ipaddress {
        10.10.110.100
    }
    track_script {
       check_apiserver
    }
}

# 所有master节点都一样
cat /etc/keepalived/check_apiserver.sh

#!/bin/bash

# if check error then repeat check for 12 times, else exit
err=0
for k in $(seq 1 12)
do
    check_code=$(curl -k https://localhost:6443)
    if [[ $check_code == "" ]]; then
        err=$(expr $err + 1)
        sleep 5
        continue
    else
        err=0
        break
    fi
done

if [[ $err != "0" ]]; then
    # if apiserver is down send SIG=1
    echo 'apiserver error!'
    exit 1
else
    # if apiserver is up send SIG=0
    echo 'apiserver normal!'
    exit 0
fi

暂停 master1 节点 
systemctl stop keepalived

查看 master2 节点是否会漂移

```

### Haproxy LB配置

```shell
# 所有的master 节点都一样

cat /etc/haproxy/haproxy.cfg
#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    # to have these messages end up in /var/log/haproxy.log you will
    # need to:
    #
    # 1) configure syslog to accept network log events.  This is done
    #    by adding the '-r' option to the SYSLOGD_OPTIONS in
    #    /etc/sysconfig/syslog
    #
    # 2) configure local2 events to go to the /var/log/haproxy.log
    #   file. A line like the following can be added to
    #   /etc/sysconfig/syslog
    #
    #    local2.*                       /var/log/haproxy.log
    #
    log         127.0.0.1 local2

    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

#---------------------------------------------------------------------
# kubernetes apiserver frontend which proxys to the backends
#---------------------------------------------------------------------
frontend kubernetes-apiserver
    mode                 tcp
    bind                 *:16443
    option               tcplog
    default_backend      kubernetes-apiserver

#---------------------------------------------------------------------
# round robin balancing between the various backends
#---------------------------------------------------------------------
backend kubernetes-apiserver
    mode        tcp
    balance     roundrobin
    server  master1 10.10.110.101:6443 check
    server  master2 10.10.110.102:6443 check
    server  master3 10.10.110.103:6443 check

#---------------------------------------------------------------------
# collection haproxy statistics message
#---------------------------------------------------------------------
listen stats
    bind                 *:1080
    stats auth           admin:123456
    stats refresh        5s
    stats realm          HAProxy\ Statistics
    stats uri            /admin?stats
    
    
# 开机自启并启动
systemctl enable haproxy && systemctl start haproxy
# 查看端口是否正常
[root@master1 ~]# ss -lnt | grep -E "16443|1080"
LISTEN     0      128          *:1080                     *:*
LISTEN     0      128          *:16443                    *:*
```

## 初始化Master节点

K8s的控制面板组件运行在Master节点上，包括etcd和API server（Kubectl便是通过API server与k8s通信）。

在执行初始化之前，我们还有一下3点需要注意：

1. 选择一个网络插件，并检查它是否需要在初始化Master时指定一些参数，比如我们可能需要根据选择的插件来设置--pod-network-cidr参数。参考：Installing a pod network add-on。
2. kubeadm使用eth0的默认网络接口（通常是内网IP）做为Master节点的advertise address，如果我们想使用不同的网络接口，可以使用`--apiserver-advertise-address=<ip-address>`参数来设置。如果适应IPv6，则必须使用IPv6的地址，如：`--apiserver-advertise-address=fd00::101`。
3. 由于国内的网络问题，建议使用kubeadm config images pull来预先拉取初始化需要用到的镜像，并检查是否能连接到gcr.io的registries。

很明显，在国内并不能访问gcr.io，我们通过修改配置文件 `imageRepository` 来指定镜像源地址(这里我使用**阿里云**的)。

在kubeadm v1.11+版本中，增加了一个kubeadm config print-default命令，可以让我们方便的将kubeadm的默认配置打印到文件中：

```yaml
[root@master1 ~]# cat kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta1
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.14.2
apiServer:
  certSANs:
    - "k8s-lb.soulmz.local"
controlPlaneEndpoint: "k8s-lb.soulmz.local:16443"
networking:
  podSubnet: "10.244.0.0/16"
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs
```

现在我们可以使用--config参数指定`kubeadm-config.yaml`文件来运行 kubeadm 的images pull的命令：

```shell
[root@master1 ~]# kubeadm config images pull --config=kubeadm-config.yaml
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.14.2
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.14.2
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.14.2
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.14.2
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.1
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.3.10
[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:1.3.1
```

初始化Master

```shell
kubeadm init --config kubeadm-config.yaml

执行下面的步骤，kubectl 即可操作
....
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

....

kubeadm join k8s-lb.soulmz.local:16443 --token yaewi6.06mdwdhf4vvgfyuz     --discovery-token-ca-cert-hash sha256:82dc07999f0f60173aa618bab989904de4bd727e8148295850e802ed21abc9d3
.....


#复制初始化的master配置到其他master节点
vim scp_master_config.sh
# on master1
IPS=(master2 master3)
for ip in ${IPS[@]};do
  ssh $ip "mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/"
  scp /etc/kubernetes/pki/ca.crt $ip:/etc/kubernetes/pki/ca.crt
  scp /etc/kubernetes/pki/ca.key $ip:/etc/kubernetes/pki/ca.key
  scp /etc/kubernetes/pki/sa.key $ip:/etc/kubernetes/pki/sa.key
  scp /etc/kubernetes/pki/sa.pub $ip:/etc/kubernetes/pki/sa.pub
  scp /etc/kubernetes/pki/front-proxy-ca.crt $ip:/etc/kubernetes/pki/front-proxy-ca.crt
  scp /etc/kubernetes/pki/front-proxy-ca.key $ip:/etc/kubernetes/pki/front-proxy-ca.key
  scp /etc/kubernetes/pki/etcd/ca.crt $ip:/etc/kubernetes/pki/etcd/ca.crt
  scp /etc/kubernetes/pki/etcd/ca.key $ip:/etc/kubernetes/pki/etcd/ca.key
  scp /etc/kubernetes/admin.conf $ip:/etc/kubernetes/admin.conf
  scp /etc/kubernetes/admin.conf $ip:~/.kube/config
done
```

- 如果没留意到 token 可以，打印加入token的命令

`kubeadm token create --print-join-command`

## Calico网络插件

为了让Pods间可以相互通信，我们必须安装一个网络插件，并且必须在部署任何应用之前安装，CoreDNS也是在网络插件安装之后才会启动的。

网络的插件完整列表，请参考 Networking and Network Policy。

在安装之前，我们先查看一下当前Pods的状态：

```shell
[root@master1 ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                        READY   STATUS     RESTARTS   AGE
kube-system   coredns-5c545769d8-5lg9x                    0/1     Pending    0          75s
kube-system   coredns-5c545769d8-6zxn6                    0/1     Pending    0          75s
kube-system   etcd-kubernetes-master                      0/1     Pending    0          5s
kube-system   kube-apiserver-kubernetes-master            0/1     Pending    0          5s
kube-system   kube-controller-manager-kubernetes-master   0/1     Pending    0          4s
kube-system   kube-proxy-x9j8p                            1/1     NodeLost   0          6m15s
kube-system   kube-scheduler-kubernetes-master            0/1     Pending    0          5s
```

如上，可以看到CoreDND的状态是Pending，就是因为我们还没有安装网络插件。

可使用如下命令命令来安装calico插件：

```shell
[root@master1 ~]# mkdir -pv calico && cd calico
[root@master1 calico]# wget https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
[root@master1 calico]# wget https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
# 这里需要修改 calico.yaml 配置里， 192.168.0.0/16 IP范围改成 10.244.0.0/16
[root@master1 calico]# kubectl apply -f .
```

稍等片刻，再使用`kubectl get pods --all-namespaces`命令来查看网络插件的安装情况：

```shell
[root@master1 ~]# kubectl get pods --all-namespaces
# 输出
NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE
kube-system   calico-node-x96gn                2/2     Running   0          47s
kube-system   coredns-78d4cf999f-6pgfr         1/1     Running   0          54m
kube-system   coredns-78d4cf999f-m9kgs         1/1     Running   0          54m
kube-system   etcd-master                      1/1     Running   3          53m
kube-system   kube-apiserver-master            1/1     Running   3          53m
kube-system   kube-controller-manager-master   1/1     Running   3          53m
kube-system   kube-proxy-mkg24                 1/1     Running   2          54m
kube-system   kube-scheduler-master            1/1     Running   3          53m
```

## 其他master节点加入

```shell
kubeadm join k8s-lb.soulmz.local:16443 --token yaewi6.06mdwdhf4vvgfyuz     --discovery-token-ca-cert-hash sha256:82dc07999f0f60173aa618bab989904de4bd727e8148295850e802ed21abc9d3 --experimental-control-plane

# 输出
....
This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Master label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.
* A new etcd member was added to the local/stacked etcd cluster.

To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.

# master节点必须执行
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

```

## master隔离

> 默认情况下，由于安全原因，集群并不会将pods部署在Master节点上。但是在开发环境下，我们可能就只有一个Master节点，这时可以使用下面的命令来解除这个限制：

```shell
kubectl taint nodes --all node-role.kubernetes.io/master-
 
# 输出
node/ubuntu1 untainted
```

## Woker节点加入

```shell
kubeadm join k8s-lb.soulmz.local:16443 --token yaewi6.06mdwdhf4vvgfyuz     --discovery-token-ca-cert-hash sha256:82dc07999f0f60173aa618bab989904de4bd727e8148295850e802ed21abc9d3

# 输出
....
This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.
```

##  k8s [kubeadm-ha]高可用集群搭建完成

```shell
[root@master1 ~]# kubectl get node,cs,pods,deploy,svc -n kube-system -o wide
NAME           STATUS   ROLES    AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION              CONTAINER-RUNTIME
node/master1   Ready    master   4h24m   v1.14.2   10.10.110.101   <none>        CentOS Linux 7 (Core)   5.1.5-1.el7.elrepo.x86_64   docker://18.9.6
node/master2   Ready    master   4h14m   v1.14.2   10.10.110.102   <none>        CentOS Linux 7 (Core)   5.1.5-1.el7.elrepo.x86_64   docker://18.9.6
node/master3   Ready    master   4h12m   v1.14.2   10.10.110.103   <none>        CentOS Linux 7 (Core)   5.1.5-1.el7.elrepo.x86_64   docker://18.9.6
node/worker1   Ready    <none>   4h11m   v1.14.2   10.10.110.104   <none>        CentOS Linux 7 (Core)   5.1.5-1.el7.elrepo.x86_64   docker://18.9.6

NAME                                 STATUS    MESSAGE             ERROR
componentstatus/scheduler            Healthy   ok
componentstatus/controller-manager   Healthy   ok
componentstatus/etcd-0               Healthy   {"health":"true"}

NAME                                  READY   STATUS    RESTARTS   AGE     IP              NODE      NOMINATED NODE   READINESS GATES
pod/calico-node-66vnf                 2/2     Running   0          4h12m   10.10.110.103   master3   <none>           <none>
pod/calico-node-bgn9k                 2/2     Running   0          4h19m   10.10.110.101   master1   <none>           <none>
pod/calico-node-fwvcc                 2/2     Running   0          4h14m   10.10.110.102   master2   <none>           <none>
pod/calico-node-pbwp8                 2/2     Running   0          4h11m   10.10.110.104   worker1   <none>           <none>
pod/coredns-8686dcc4fd-4m5wj          1/1     Running   0          4h24m   10.244.0.3      master1   <none>           <none>
pod/coredns-8686dcc4fd-j5nkg          1/1     Running   0          4h24m   10.244.0.2      master1   <none>           <none>
pod/etcd-master1                      1/1     Running   0          4h23m   10.10.110.101   master1   <none>           <none>
pod/etcd-master2                      1/1     Running   0          4h14m   10.10.110.102   master2   <none>           <none>
pod/etcd-master3                      1/1     Running   0          4h12m   10.10.110.103   master3   <none>           <none>
pod/kube-apiserver-master1            1/1     Running   0          4h23m   10.10.110.101   master1   <none>           <none>
pod/kube-apiserver-master2            1/1     Running   0          4h14m   10.10.110.102   master2   <none>           <none>
pod/kube-apiserver-master3            1/1     Running   0          4h12m   10.10.110.103   master3   <none>           <none>
pod/kube-controller-manager-master1   1/1     Running   1          4h23m   10.10.110.101   master1   <none>           <none>
pod/kube-controller-manager-master2   1/1     Running   0          4h14m   10.10.110.102   master2   <none>           <none>
pod/kube-controller-manager-master3   1/1     Running   0          4h12m   10.10.110.103   master3   <none>           <none>
pod/kube-proxy-74tdf                  1/1     Running   0          4h11m   10.10.110.104   worker1   <none>           <none>
pod/kube-proxy-87nzh                  1/1     Running   0          4h24m   10.10.110.101   master1   <none>           <none>
pod/kube-proxy-chbqd                  1/1     Running   0          4h14m   10.10.110.102   master2   <none>           <none>
pod/kube-proxy-v9x62                  1/1     Running   0          4h12m   10.10.110.103   master3   <none>           <none>
pod/kube-scheduler-master1            1/1     Running   1          4h23m   10.10.110.101   master1   <none>           <none>
pod/kube-scheduler-master2            1/1     Running   0          4h14m   10.10.110.102   master2   <none>           <none>
pod/kube-scheduler-master3            1/1     Running   0          4h12m   10.10.110.103   master3   <none>           <none>

NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS     IMAGES                                                  SELECTOR
deployment.extensions/calico-typha   0/0     0            0           4h19m   calico-typha   calico/typha:v3.3.6                                     k8s-app=calico-typha
deployment.extensions/coredns        2/2     2            2           4h24m   coredns        registry.aliyuncs.com/google_containers/coredns:1.3.1   k8s-app=kube-dns

NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE     SELECTOR
service/calico-typha   ClusterIP   10.101.111.132   <none>        5473/TCP                 4h19m   k8s-app=calico-typha
service/kube-dns       ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP   4h24m   k8s-app=kube-dns
```

## 验证下K8s集群是否正常

- 首先验证kube-apiserver, kube-controller-manager, kube-scheduler, pod network 是否正常：

```shell
[root@master1 ~]# kubectl create deployment nginx --image=nginx:alpine
deployment.apps/nginx created
[root@master1 ~]# kubectl get pods -l app=nginx -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP           NODE          NOMINATED NODE
nginx-65d5c4f7cc-fgj2f   1/1     Running   0          52s   10.244.6.2   master1   <none>
nginx-65d5c4f7cc-lq585   1/1     Running   0          23s   10.244.8.2   master2   <none>
[root@master1 ~]# kubectl scale deployment nginx --replicas=2
deployment.extensions/nginx scaled
```

- 再验证一下kube-proxy是否正常：

```shell
[root@master1 ~]# kubectl expose deployment nginx --port=80 --type=NodePort
service/nginx exposed
# 查看暴露的服务端口
[root@master1 ~]# kubectl get services nginx
NAME    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
nginx   NodePort   10.98.173.217   <none>        80:30879/TCP   9s


# 可以通过任意 NodeIP:Port 在集群外部访问这个服务，本示例中部署的2台集群IP分别是10.10.110.101和10.10.110.102、10.10.110.103、10.10.110.104
curl http://10.10.110.101:30879
curl http://10.10.110.102:30879
curl http://10.10.110.103:30879

```

- 最后验证一下dns, pod network是否正常：

```shell
[root@master1 ~]# kubectl run -it curl --image=radial/busyboxplus:curl
kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.
If you don't see a command prompt, try pressing enter.
# 解析nginx(pod)的
[ root@curl-5cc7b478b6-ftmcl:/ ]$ nslookup nginx
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      nginx
Address 1: 10.98.173.217 nginx.default.svc.cluster.local

#测试IP
[ root@curl-5cc7b478b6-ftmcl:/ ]$ curl http://10.98.173.217
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

# 测试dns
[ root@curl-5cc7b478b6-ftmcl:/ ]$ curl http://nginx.default.svc.cluster.local
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

验证通过，集群搭建成功，接下来我们就可以参考官方文档来部署其他服务，愉快的玩耍了。

## 卸载集群

想要撤销kubeadm执行的操作，首先要排除节点，并确保该节点为空, 然后再将其关闭。

在Master节点上运行：

```shell
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>
```

然后在需要移除的节点上，重置kubeadm的安装状态：

```shell
kubeadm reset -f

ifconfig cni0 down

ip link delete cni0

ifconfig flannel.1 down

ip link delete flannel.1

rm -rf /var/lib/cni/
```

如果你想重新配置集群，使用新的参数重新运行kubeadm init或者kubeadm join即可。

- 如果需要从其它任意节点控制集群，则需要复制 Master 的安全配置信息到每台服务器

```shell
$ mkdir -p $HOME/.kube
$ scp root@10.10.110.101:/etc/kubernetes/admin.conf $HOME/.kube/config
$ chown $(id -u):$(id -g) $HOME/.kube/config
$ kubectl get nodes
```



完。